{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Jmrs_T4CjW"
      },
      "source": [
        "# Behind the pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKiZuRSN4CjW"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYf0hdxr4CjW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TvlOOg04CjX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing with a tokenizer"
      ],
      "metadata": {
        "id": "QCbCoL4FI0FC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like other neural networks, Transformer models can’t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a tokenizer, which will be responsible for:\n",
        "\n",
        "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
        "- Mapping each token to an integer\n",
        "- Adding additional inputs that may be useful to the model\n",
        "\n",
        "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the Model Hub. To do this, we use the AutoTokenizer class and its from_pretrained() method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache it (so it’s only downloaded the first time you run the code below)."
      ],
      "metadata": {
        "id": "l4eVY8CN9l48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AnhXHDC4CjX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the tokenizer, we can directly pass our sentences to it and we’ll get back a dictionary that’s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
        "\n",
        "To specify the type of tensors we want to get back (PyTorch or plain NumPy), we use the return_tensors argument:"
      ],
      "metadata": {
        "id": "AcMuoQsC-fDO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Xp7BU94CjX"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going through the model"
      ],
      "metadata": {
        "id": "4fgkMzNpI519"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download our pretrained model the same way we did with our tokenizer.\n",
        "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). This can be provided by the AutoModelForSequenceClassification:"
      ],
      "metadata": {
        "id": "Kx2sV9iABbLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIeoI3y54CjY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"
      ],
      "metadata": {
        "id": "edotIhk_F-Vy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUakGHy_4CjZ"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "_1yDfpZkGEp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing the output"
      ],
      "metadata": {
        "id": "_i-dyU1sJUh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values we get as output from our model don’t necessarily make sense by themselves. Let’s take a look:"
      ],
      "metadata": {
        "id": "rqTj31rYGRMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDO_jHmW4CjZ"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all HF Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
      ],
      "metadata": {
        "id": "Vdi8kPtWGUpc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHuTv7Bp4CjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that the model predicted for both senteces. These are recognizable probability scores.\n",
        "\n",
        "To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):"
      ],
      "metadata": {
        "id": "sUxEqA5pGmbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rqlLicx4CjZ"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models\n"
      ],
      "metadata": {
        "id": "8iGjJcb54GVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Transformer"
      ],
      "metadata": {
        "id": "HKpvgSv_Jj1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "xUmsXJMA4Kp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the tokenizer, the from_pretrained() method will download and cache the model data from the Hugging Face Hub. As mentioned previously, the checkpoint name corresponds to a specific model architecture and weights, in this case a BERT model with a basic architecture (12 layers, 768 hidden size, 12 attention heads) and cased inputs (meaning that the uppercase/lowercase distinction is important). There are many checkpoints available on the Hub."
      ],
      "metadata": {
        "id": "nIOq52VcHwOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving methods"
      ],
      "metadata": {
        "id": "Pa2MMTc8Jy97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving a model is as simple as saving a tokenizer. In fact, the models actually have the same save_pretrained() method, which saves the model’s weights and architecture configuration:"
      ],
      "metadata": {
        "id": "B6asjQ0OIPHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "zKMcAIa94t2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "OvU7a0FD443W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we’ll explore exactly what happens in the tokenization pipeline."
      ],
      "metadata": {
        "id": "7NGO7pZwJAZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Python split function\n",
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "kE6-CbtE47Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading and saving"
      ],
      "metadata": {
        "id": "teJVoYkyKg4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods:\n",
        "\n",
        "from_pretrained() and save_pretrained(). These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:"
      ],
      "metadata": {
        "id": "XJfI6IIRJLXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "nYSwlODc5QEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
      ],
      "metadata": {
        "id": "AtoEa_p5JaLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "fZlLZ45t5R5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ],
      "metadata": {
        "id": "UH5gfO2c5bnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving a tokenizer is identical to saving a model:"
      ],
      "metadata": {
        "id": "kxwDtZI6Jmjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "NrBrqdpMKoJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "2d8OUMij5jRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
        "\n",
        "The first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
        "\n",
        "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained."
      ],
      "metadata": {
        "id": "FfwxgLS2Jt2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "The tokenization process is done by the tokenize() method of the tokenizer:\n"
      ],
      "metadata": {
        "id": "fWaBvv44KMlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "rKgBx1RE5hDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From tokens to input IDs\n",
        "\n",
        "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:"
      ],
      "metadata": {
        "id": "sz7pb4CVKhaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "bE131r-55o1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding\n",
        "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows:"
      ],
      "metadata": {
        "id": "Cw1nyFRS5rmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "id": "YmKhnCJJ5s1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multiple sequences"
      ],
      "metadata": {
        "id": "iKIz183I9Blp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s convert this list of numbers to a tensor and send it to the model:"
      ],
      "metadata": {
        "id": "JUXiXxJILpOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n"
      ],
      "metadata": {
        "id": "P0865c3O9IQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ],
      "metadata": {
        "id": "ITlrQ7eN9Qnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is that we sent a single sequence to the model, whereas HF Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence. But if you look closely, you’ll see that the tokenizer didn’t just convert the list of input IDs into a tensor, it added a dimension on top of it:"
      ],
      "metadata": {
        "id": "-9xfAJ-mLrWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "5Pd-FOl-9S-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try again and add a new dimension:"
      ],
      "metadata": {
        "id": "0AY5HTzNL2Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "id": "npBgsidq9YLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:"
      ],
      "metadata": {
        "id": "aIgKYKhIMkLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [ids, ids]"
      ],
      "metadata": {
        "id": "5fK52R7c9g8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batching allows the model to work when we feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There’s a second issue, though. When we're trying to batch together two (or more) sentences, they might be of different lengths. Therefore, we need to use padding to create the same length in order to create a rectangular tensor from them."
      ],
      "metadata": {
        "id": "apVwlThsMndl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "qeMAF_ft9ipP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following list of lists cannot be converted to a tensor:"
      ],
      "metadata": {
        "id": "j5Oi9U4MNVif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ],
      "metadata": {
        "id": "XYtCUSu99iQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to work around this, we’ll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values."
      ],
      "metadata": {
        "id": "KPSw-FAvNb62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ],
      "metadata": {
        "id": "FoaODt6G9nq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The padding token ID can be found in tokenizer.pad_token_id. Let’s use it and send our two sentences through the model individually and batched together:"
      ],
      "metadata": {
        "id": "RHZICoJGNgE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ],
      "metadata": {
        "id": "yP0Q1PUt9qMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There’s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we’ve got completely different values!\n",
        "\n",
        "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
      ],
      "metadata": {
        "id": "fNRYcc8SNs9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attetnion mask"
      ],
      "metadata": {
        "id": "7p3jVvM69uA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
        "\n",
        "Let’s complete the previous example with an attention mask:"
      ],
      "metadata": {
        "id": "wJzC7NgEOlN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "vPrTgEOa9tV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get the same logits for the second sentence in the batch."
      ],
      "metadata": {
        "id": "UiKZJqfeO6MJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "UB80nuj_8wL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last few sections, we've been trying our best to do most of the work by hand. We've explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.\n",
        "\n",
        "However, the HF Transformers API can handle all of this for us with a high-level function that we'll dive into here. When you call your `tokenizer` directly on the sentence, you get back inputs that are ready to pass through your model:"
      ],
      "metadata": {
        "id": "gAmaZkQgPfnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ],
      "metadata": {
        "id": "iuaFj8nE8yIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the `model_inputs` variable contains everything that's necessary for a model to operate well. For DistilBERT, that includes the input IDs as well as the attention mask. Other models that accept additional inputs will also have those output by the `tokenizer` object.\n",
        "\n",
        "\n",
        "As we'll see in some examples below, this method is very powerful. First, it can tokenize a single sequence:"
      ],
      "metadata": {
        "id": "ij6yhVW6QQkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)"
      ],
      "metadata": {
        "id": "_TyQXVD-Qont"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It also handles multiple sequences at a time, with no change in the API:\n"
      ],
      "metadata": {
        "id": "W3qEfidZQrV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)"
      ],
      "metadata": {
        "id": "Lprc41Zy9Ayk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can pad according to several objectives:"
      ],
      "metadata": {
        "id": "4hiifTIoQ3S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "\n",
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "\n",
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
      ],
      "metadata": {
        "id": "RVGbFnxi-Jku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can also truncate sequences:"
      ],
      "metadata": {
        "id": "EGzc_7jPQ6PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
      ],
      "metadata": {
        "id": "8PR6rkcb-M-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tokenizer` object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks — `\"pt\"` returns PyTorch tensors and `\"np\"` returns NumPy arrays:"
      ],
      "metadata": {
        "id": "AKF9CXfTQ_pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "# Returns PyTorch tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Returns TensorFlow tensors\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Returns NumPy arrays\n",
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
      ],
      "metadata": {
        "id": "O3sZoToR-TuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Special tokens"
      ],
      "metadata": {
        "id": "_ATf_Ioe-jIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:"
      ],
      "metadata": {
        "id": "Z1ALyI7IRKCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "rYWMltrp-dV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One token ID was added at the beginning, and one at the end. Let’s decode the two sequences of IDs above to see what this is about:"
      ],
      "metadata": {
        "id": "3Te2fPGlROWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "id": "HrB7B8DO-gT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer added the special word `[CLS]` at the beginning and the special word `[SEP]` at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don't add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you."
      ],
      "metadata": {
        "id": "hqOAm8x_RSJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wrapping up: From tokenizer to model"
      ],
      "metadata": {
        "id": "UXsWqppo-lsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've seen all the individual steps the `tokenizer` object uses when applied on texts, let's see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API:"
      ],
      "metadata": {
        "id": "mcMOlIzcRWIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)"
      ],
      "metadata": {
        "id": "mmjhofMi-n2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtVJ-oXnT9Lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F2Jmrs_T4CjW",
        "QCbCoL4FI0FC",
        "4fgkMzNpI519",
        "8iGjJcb54GVv",
        "HKpvgSv_Jj1T",
        "Pa2MMTc8Jy97",
        "ePhjtV1eJ792",
        "OvU7a0FD443W",
        "teJVoYkyKg4q",
        "2d8OUMij5jRF",
        "Cw1nyFRS5rmy",
        "iKIz183I9Blp",
        "qeMAF_ft9ipP",
        "7p3jVvM69uA5",
        "UB80nuj_8wL4",
        "_ATf_Ioe-jIz",
        "UXsWqppo-lsE"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}